{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Estados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que haremos aquí es recopilar los datos de las reviews de los estados que utilizaremos en el análisis. Los archivos en crudo se presentaron en formato Json, en archivos individuales para cada uno de los 51 estados de EEUU, por lo que procederemos a ordenar los mismos, en formato parquet, y por la dimension, en dataframes individuales, para luego crear una tabla única para los 5 estados pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New York"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'NORESTE\\review-New_York'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\NORESTE_parquet\\Rev-New_York.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_New_York_1 = pd.read_parquet(r'NORESTE_parquet\\Rev-New_York.parquet')\n",
    "\n",
    "print(Rev_New_York_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\NORESTE_parquet\\Rev-New_York.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_New_York = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_ny = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_ny)\n",
    "        df_list_New_York.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_new_york = pd.concat(df_list_New_York, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_new_york.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_york = df_new_york.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_new_york['user_id'] = df_new_york['user_id'].astype('str')\n",
    "print(df_new_york['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['time'] = pd.to_datetime(df_new_york['time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_new_york[['resp_text', 'resp_time']] = df_new_york['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_york['resp_time'] = pd.to_datetime(df_new_york['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_new_york['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_new_york = df_new_york.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_new_york = df_new_york.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_new_york.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_new_york = df_new_york[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental\n",
    "df_new_york.insert(0, 'review_id', range(1, 1 + len(df_new_york)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_new_york.insert(2, 'Estado', 'New York')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_new_york.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Mexico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pasan de formato Json a Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\OESTE\\review-New_Mexico'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\OESTE.parquet\\Rev-New_Mexico.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_New_Mexico_1 = pd.read_parquet(r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\OESTE.parquet\\Rev-New_Mexico.parquet')\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(Rev_New_Mexico_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos .parquet para formar un Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\OESTE.parquet\\Rev-New_Mexico.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_New_Mexico = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_nmex = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_nmex)\n",
    "        df_list_New_Mexico.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_new_mexico = pd.concat(df_list_New_Mexico, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_new_mexico.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_mexico = df_new_mexico.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_new_mexico['user_id'] = df_new_mexico['user_id'].astype('str')\n",
    "print(df_new_mexico['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_mexico['time'] = pd.to_datetime(df_new_mexico['time'], unit='ms').dt.normalize()\n",
    "print(df_new_mexico['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_new_mexico[['resp_text', 'resp_time']] = df_new_mexico['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_new_mexico['resp_time'] = pd.to_datetime(df_new_mexico['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_new_mexico['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_new_mexico = df_new_mexico.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_new_mexico = df_new_mexico.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_new_mexico.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_new_mexico = df_new_mexico[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 2700001 para continuar desde el df_new_york\n",
    "df_new_mexico.insert(0, 'review_id', range(2700001, 2700001 + len(df_new_mexico)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_new_mexico.insert(2, 'Estado', 'New Mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_new_mexico.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pennsylvania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten los archivos Json a formato parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta de entrada que contiene los archivos JSON\n",
    "input_folder = r'NORESTE\\review-Pennsylvania'\n",
    "# Carpeta de salida para guardar los archivos Parquet\n",
    "output_folder = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\NORESTE_parquet\\Rev-Pennsylvania.parquet'\n",
    "\n",
    "# Crear la carpeta de salida si no existe\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        # Ruta completa del archivo JSON\n",
    "        json_file_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # Leer el archivo JSON\n",
    "        try:\n",
    "            df = pd.read_json(json_file_path, lines=True)\n",
    "            \n",
    "            # Crear nombre de archivo Parquet (sin la extensión .json)\n",
    "            parquet_file_path = os.path.join(output_folder, filename.replace('.json', '.parquet'))\n",
    "            \n",
    "            # Guardar el DataFrame como archivo Parquet\n",
    "            df.to_parquet(parquet_file_path, index=False, compression='snappy', engine='pyarrow')\n",
    "            \n",
    "            print(f\"Convertido: {filename} a {parquet_file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al convertir {filename}: {e}\")\n",
    "\n",
    "print(\"Conversión completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo Parquet\n",
    "Rev_Pennsylvania_1 = pd.read_parquet(r'NORESTE_parquet\\Rev-Pennsylvania.parquet')\n",
    "\n",
    "print(Rev_Pennsylvania_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se concatenan los archivos parquet para formar un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta que contiene los archivos Parquet\n",
    "input_folder_parquet = r'C:\\Users\\debor\\OneDrive\\Documentos\\Henry\\PROYECTO FINAL local\\NORESTE_parquet\\Rev-Pennsylvania.parquet'\n",
    "\n",
    "# Lista para almacenar DataFrames\n",
    "df_list_Pennsylvania = []\n",
    "\n",
    "# Iterar sobre todos los archivos Parquet en la carpeta de entrada\n",
    "for filename in os.listdir(input_folder_parquet):\n",
    "    if filename.endswith('.parquet'):\n",
    "        # Ruta completa del archivo Parquet\n",
    "        parquet_file_path_psv = os.path.join(input_folder_parquet, filename)\n",
    "        \n",
    "        # Leer el archivo Parquet y añadir el DataFrame a la lista\n",
    "        df = pd.read_parquet(parquet_file_path_psv)\n",
    "        df_list_Pennsylvania.append(df)\n",
    "\n",
    "# Concatenar todos los DataFrames en uno solo\n",
    "df_pennsylvania = pd.concat(df_list_Pennsylvania, ignore_index=True)\n",
    "\n",
    "# Verificar el DataFrame combinado\n",
    "print(df_pennsylvania.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina columna irrelevante para el análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pennsylvania = df_pennsylvania.drop(columns=['pics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se convierten las columnas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'user_id' a tipo object\n",
    "df_pennsylvania['user_id'] = df_pennsylvania['user_id'].astype('str')\n",
    "print(df_pennsylvania['user_id'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['time'] = pd.to_datetime(df_pennsylvania['time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se desempaqueta la columna 'resp' y se agregan dos columnas al df\n",
    "df_pennsylvania[['resp_text', 'resp_time']] = df_pennsylvania['resp'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte la columna 'resp_time' a datetime y luego a solo fecha manteniendo tipo datetime\n",
    "df_pennsylvania['resp_time'] = pd.to_datetime(df_pennsylvania['resp_time'], unit='ms').dt.normalize()\n",
    "print(df_pennsylvania['resp_time'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se elimina la columna 'resp' original\n",
    "df_pennsylvania = df_pennsylvania.drop(columns=['resp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se renombra la columna gmap_id como 'id_negocio', para mayor comprensión en el análisis\n",
    "df_pennsylvania = df_pennsylvania.rename(columns={'gmap_id': 'id_negocio'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reordena la columna 'id_negocio'\n",
    "columnas = list(df_pennsylvania.columns)\n",
    "columnas.insert(0, columnas.pop(columnas.index('id_negocio')))\n",
    "df_pennsylvania = df_pennsylvania[columnas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'review_id' autoincremental desde 4500001 para continuar desde el df_new_mexico\n",
    "df_pennsylvania.insert(0, 'review_id', range(4500001, 4500001 + len(df_pennsylvania)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se añade la columna 'Estado'\n",
    "df_pennsylvania.insert(2, 'Estado', 'Pennsylvania')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se verifica como queda el dataframe\n",
    "print(df_pennsylvania.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se concatenan los tres DataFrames anteriores para formar uno solo\n",
    "df_tres_estados = pd.concat([df_new_york, df_new_mexico, df_pennsylvania])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_tres_estados.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_por_columna = df_tres_estados.isnull().sum()\n",
    "print(nulos_por_columna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AAentorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
